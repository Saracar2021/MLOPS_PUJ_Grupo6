"""
API principal para el servicio de clasificación de pingüinos Palmer.

Este módulo orquesta todos los componentes del sistema ML (procesamiento, modelos, predicción)
en una interfaz REST coherente. Implementa patrones de diseño robustos para manejo de errores,
logging, y observabilidad que son esenciales en sistemas de ML en producción.
"""

import sys
import os
from pathlib import Path

# Agregar el directorio padre al path para permitir imports relativos
sys.path.append(str(Path(__file__).parent.parent))

from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import numpy as np
import logging
import time
from datetime import datetime
from typing import Optional
import uuid

# Imports de nuestros módulos personalizados
from src.model_manager import ModelManager
from api.schemas import (
    PenguinFeaturesSimple, 
    PenguinFeaturesComplete, 
    PredictionResponse,
    HealthResponse,
    ModelInfoResponse,
    ErrorResponse
)

# Configurar logging global para el API
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('api.log')
    ]
)
logger = logging.getLogger(__name__)

# Inicializar la aplicación FastAPI con metadata completa
app = FastAPI(
    title="Palmer Penguins Species Classifier API",
    description="""
    API RESTful para clasificación de especies de pingüinos usando el dataset Palmer Penguins.
    
    Este servicio proporciona endpoints para:
    - Predicción de especies basada en características físicas
    - Verificación de salud del servicio
    - Información detallada sobre el modelo cargado
    
    El modelo utiliza regresión logística entrenada en datos de pingüinos de las islas
    Palmer en Antarctica, clasificando entre especies Adelie, Chinstrap, y Gentoo.
    """,
    version="1.0.0",
    contact={
        "name": "MLOps Team",
        "email": "mlops@university.edu"
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT"
    }
)

# Configurar CORS para permitir requests desde diferentes orígenes
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producción, especificar orígenes específicos
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Variables globales para gestión de estado del modelo
model_manager: Optional[ModelManager] = None
cached_model = None
cached_scaler = None
cached_metadata = None
model_load_time = None

# Dependencia para request tracking y logging
async def log_request_info(request: Request):
    """
    Dependency que registra información de cada request para observabilidad.
    
    En sistemas de producción, este tipo de logging es crucial para
    debugging, monitoring, y análisis de patrones de uso.
    """
    request_id = str(uuid.uuid4())[:8]
    start_time = time.time()
    
    logger.info(f"[{request_id}] {request.method} {request.url.path} - Iniciando request")
    
    # Agregar información al contexto del request para uso posterior
    request.state.request_id = request_id
    request.state.start_time = start_time
    
    return request_id


@app.on_event("startup")
async def load_model_artifacts():
    """
    Event handler que se ejecuta al iniciar la aplicación.
    
    Este patrón de carga durante startup es crucial en MLOps porque:
    1. Detecta problemas de modelo temprano en el ciclo de vida del servicio
    2. Evita latencia de primera request (cold start)
    3. Permite validación de integridad antes de servir traffic
    """
    global model_manager, cached_model, cached_scaler, cached_metadata, model_load_time
    
    logger.info("=== Iniciando carga de artefactos del modelo ===")
    
    try:
        # Inicializar el gestor de modelos
        model_manager = ModelManager(models_dir="models")
        
        # Intentar cargar todos los artefactos
        start_time = time.time()
        cached_model, cached_scaler, cached_metadata = model_manager.load_model_artifacts()
        load_duration = time.time() - start_time
        model_load_time = datetime.now().isoformat()
        
        logger.info(f"Artefactos cargados exitosamente en {load_duration:.2f} segundos")
        logger.info(f"Modelo: {cached_metadata.get('model_info', {}).get('algorithm', 'unknown')}")
        logger.info(f"Accuracy: {cached_metadata.get('performance_metrics', {}).get('accuracy', 'unknown')}")
        
    except FileNotFoundError as e:
        logger.error(f"Artefactos del modelo no encontrados: {e}")
        logger.warning("API iniciará sin modelo cargado - solo endpoints de información disponibles")
    except Exception as e:
        logger.error(f"Error cargando artefactos del modelo: {e}")
        logger.warning("API iniciará sin modelo cargado")


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """
    Handler personalizado para excepciones HTTP que proporciona logging estructurado.
    """
    request_id = getattr(request.state, 'request_id', 'unknown')
    
    logger.warning(f"[{request_id}] HTTP {exc.status_code}: {exc.detail}")
    
    return JSONResponse(
        status_code=exc.status_code,
        content=ErrorResponse(
            error=f"HTTP{exc.status_code}",
            message=exc.detail,
            timestamp=datetime.now().isoformat(),
            request_id=request_id
        ).dict()
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """
    Handler para excepciones no anticipadas que proporciona logging detallado.
    """
    request_id = getattr(request.state, 'request_id', 'unknown')
    
    logger.error(f"[{request_id}] Error no manejado: {type(exc).__name__}: {str(exc)}")
    
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            error="InternalServerError",
            message="Error interno del servidor",
            timestamp=datetime.now().isoformat(),
            request_id=request_id
        ).dict()
    )


def convert_simple_to_complete_features(simple_features: PenguinFeaturesSimple) -> PenguinFeaturesComplete:
    """
    Converter características simples a formato completo con one-hot encoding.
    
    Esta función encapsula la lógica de transformación que permite que nuestro API
    ofrezca una interfaz user-friendly mientras internamente usa la representación
    que espera el modelo entrenado.
    """
    # Inicializar todas las variables categóricas en 0
    island_encoding = {"island_Biscoe": 0, "island_Dream": 0, "island_Torgersen": 0}
    sex_encoding = {"sex_Female": 0, "sex_Male": 0}
    
    # Aplicar one-hot encoding para isla
    island_key = f"island_{simple_features.island.value}"
    if island_key in island_encoding:
        island_encoding[island_key] = 1
    else:
        raise HTTPException(
            status_code=400, 
            detail=f"Isla no válida: {simple_features.island}. Valores permitidos: Biscoe, Dream, Torgersen"
        )
    
    # Aplicar one-hot encoding para sexo
    sex_key = f"sex_{simple_features.sex.value}"
    if sex_key in sex_encoding:
        sex_encoding[sex_key] = 1
    else:
        raise HTTPException(
            status_code=400, 
            detail=f"Sexo no válido: {simple_features.sex}. Valores permitidos: Male, Female"
        )
    
    # Construir objeto completo
    return PenguinFeaturesComplete(
        bill_length_mm=simple_features.bill_length_mm,
        bill_depth_mm=simple_features.bill_depth_mm,
        flipper_length_mm=simple_features.flipper_length_mm,
        body_mass_g=simple_features.body_mass_g,
        **island_encoding,
        **sex_encoding
    )


def get_species_name(species_code: int) -> str:
    """
    Convertir código numérico de especie a nombre legible.
    
    Esta función proporciona la traducción inversa del mapeo creado
    durante entrenamiento, permitiendo respuestas user-friendly.
    """
    species_mapping = {1: "Adelie", 2: "Chinstrap", 3: "Gentoo"}
    return species_mapping.get(species_code, "Unknown")


def validate_model_availability():
    """
    Dependency function que valida que el modelo esté disponible.
    
    Este patrón de validación como dependency permite reutilización
    across múltiples endpoints que requieren modelo cargado.
    """
    if cached_model is None or cached_scaler is None:
        raise HTTPException(
            status_code=503, 
            detail="Modelo no disponible. El servicio se está iniciando o hay un problema con los artefactos del modelo."
        )


@app.get("/", response_model=dict)
async def root(request_id: str = Depends(log_request_info)):
    """
    Endpoint raíz que proporciona información general sobre el API.
    
    Este endpoint actúa como "health check" básico y punto de entrada
    para desarrolladores explorando el API.
    """
    return {
        "service": "Palmer Penguins Species Classifier",
        "version": "1.0.0",
        "status": "active",
        "model_loaded": cached_model is not None,
        "model_load_time": model_load_time,
        "endpoints": {
            "/predict/simple": "Predicción con entrada user-friendly",
            "/predict/complete": "Predicción con one-hot encoding explícito",
            "/health": "Estado detallado del servicio",
            "/model/info": "Información sobre el modelo cargado",
            "/docs": "Documentación interactiva (Swagger UI)",
            "/redoc": "Documentación alternativa (ReDoc)"
        },
        "request_id": request_id
    }


@app.get("/health", response_model=HealthResponse)
async def health_check(request_id: str = Depends(log_request_info)):
    """
    Endpoint de salud comprehensivo para monitoring y orchestration.
    
    Este endpoint es fundamental para sistemas de producción donde
    tools como Kubernetes necesitan verificar la salud del servicio.
    """
    # Determinar estado general basado en disponibilidad de componentes
    model_available = cached_model is not None
    scaler_available = cached_scaler is not None
    overall_healthy = model_available and scaler_available
    
    status = "healthy" if overall_healthy else "degraded"
    
    return HealthResponse(
        status=status,
        model_loaded=model_available,
        scaler_loaded=scaler_available,
        timestamp=datetime.now().isoformat(),
        version="1.0.0"
    )


@app.get("/model/info", response_model=ModelInfoResponse)
async def get_model_info(
    request_id: str = Depends(log_request_info),
    _: None = Depends(validate_model_availability)
):
    """
    Obtener información detallada sobre el modelo cargado.
    
    Este endpoint es crucial para debugging, auditing, y verificación
    de que el modelo correcto está siendo usado en producción.
    """
    try:
        # Extraer información del metadata cacheado
        model_info = cached_metadata.get("model_info", {})
        performance = cached_metadata.get("performance_metrics", {})
        feature_info = cached_metadata.get("feature_info", {})
        data_info = cached_metadata.get("data_info", {})
        
        return ModelInfoResponse(
            model_type=model_info.get("algorithm", "unknown"),
            version=cached_metadata.get("model_version", "unknown"),
            training_date=cached_metadata.get("training_timestamp", "unknown"),
            accuracy=performance.get("accuracy", 0.0),
            feature_count=feature_info.get("num_features", 0),
            target_classes={str(k): v for k, v in data_info.get("species_mapping", {}).items()},
            features=feature_info.get("feature_columns", [])
        )
        
    except Exception as e:
        logger.error(f"[{request_id}] Error obteniendo información del modelo: {e}")
        raise HTTPException(status_code=500, detail="Error obteniendo información del modelo")


@app.post("/predict/simple", response_model=PredictionResponse)
async def predict_species_simple(
    features: PenguinFeaturesSimple,
    request_id: str = Depends(log_request_info),
    _: None = Depends(validate_model_availability)
):
    """
    Predecir especie usando entrada simplificada con conversión automática.
    
    Este endpoint proporciona la experiencia más user-friendly al manejar
    automáticamente la conversión de valores categóricos legibles a la
    representación interna que requiere el modelo.
    """
    try:
        start_inference = time.time()
        
        # Convertir a formato completo
        complete_features = convert_simple_to_complete_features(features)
        
        # Ejecutar predicción usando el endpoint interno
        result = await execute_prediction(complete_features, request_id, start_inference)
        
        logger.info(f"[{request_id}] Predicción simple exitosa: {result.species}")
        return result
        
    except HTTPException:
        # Re-lanzar excepciones HTTP específicas
        raise
    except Exception as e:
        logger.error(f"[{request_id}] Error en predicción simple: {e}")
        raise HTTPException(status_code=500, detail="Error ejecutando predicción")


@app.post("/predict/complete", response_model=PredictionResponse)
async def predict_species_complete(
    features: PenguinFeaturesComplete,
    request_id: str = Depends(log_request_info),
    _: None = Depends(validate_model_availability)
):
    """
    Predecir especie usando características con one-hot encoding explícito.
    
    Este endpoint está diseñado para integraciones avanzadas donde el cliente
    prefiere manejar la codificación categórica directamente, proporcionando
    mayor control sobre la entrada al modelo.
    """
    try:
        start_inference = time.time()
        result = await execute_prediction(features, request_id, start_inference)
        
        logger.info(f"[{request_id}] Predicción completa exitosa: {result.species}")
        return result
        
    except Exception as e:
        logger.error(f"[{request_id}] Error en predicción completa: {e}")
        raise HTTPException(status_code=500, detail="Error ejecutando predicción")


async def execute_prediction(features: PenguinFeaturesComplete, request_id: str, start_time: float) -> PredictionResponse:
    """
    Función helper que ejecuta la predicción actual del modelo.
    
    Esta separación permite reutilización de lógica entre diferentes
    endpoints de predicción mientras mantiene logging y error handling consistentes.
    """
    try:
        # Convertir características a array numpy en el orden correcto
        feature_array = np.array([[
            features.bill_length_mm,
            features.bill_depth_mm,
            features.flipper_length_mm,
            features.body_mass_g,
            features.island_Biscoe,
            features.island_Dream,
            features.island_Torgersen,
            features.sex_Female,
            features.sex_Male
        ]])
        
        # Aplicar escalado usando el mismo scaler del entrenamiento
        feature_array_scaled = cached_scaler.transform(feature_array)
        
        # Ejecutar predicción
        prediction = cached_model.predict(feature_array_scaled)[0]
        probabilities = cached_model.predict_proba(feature_array_scaled)[0]
        
        # Calcular confianza y crear diccionario de probabilidades
        confidence = float(max(probabilities))
        prob_dict = {
            "Adelie": float(probabilities[0]),
            "Chinstrap": float(probabilities[1]),
            "Gentoo": float(probabilities[2])
        }
        
        # Obtener nombre de especie
        species_name = get_species_name(int(prediction))
        
        # Calcular tiempo de procesamiento
        processing_time = (time.time() - start_time) * 1000  # Convertir a milliseconds
        
        # Crear metadata de predicción
        prediction_metadata = {
            "processing_time_ms": round(processing_time, 2),
            "model_version": cached_metadata.get("model_version", "unknown"),
            "request_timestamp": datetime.now().isoformat()
        }
        
        return PredictionResponse(
            species=species_name,
            species_code=int(prediction),
            confidence=confidence,
            probabilities=prob_dict,
            prediction_metadata=prediction_metadata
        )
        
    except Exception as e:
        logger.error(f"[{request_id}] Error en execute_prediction: {e}")
        raise


if __name__ == "__main__":
    """
    Entry point para ejecutar el servidor directamente.
    
    Esta configuración permite ejecutar el API tanto directamente
    como through un WSGI server en producción.
    """
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8989,
        reload=True,  # Solo para desarrollo
        log_level="info"
    )
